{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  EMF RBM Energy Densities\n",
    "\n",
    "Look at different ways of computing the\n",
    "\n",
    "- energy density\n",
    "- free energy density\n",
    "- entropy density\n",
    "\n",
    "We can\n",
    "\n",
    "- use the TAP S and MF Energy *U_naive   \n",
    "    - which takes an equlibration step first  \n",
    "    - S(mv, mh), U_naive(mv, mh)\n",
    "- use a simple h = means_hiddens() step\n",
    "    - S(v, h), E_MF(v, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:21:52.935465",
     "start_time": "2016-11-08T14:21:50.119350"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charlesmartin14/anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn import linear_model, datasets, metrics, preprocessing \n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:21:52.945369",
     "start_time": "2016-11-08T14:21:52.937252"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils.validation import assert_all_finite\n",
    "from scipy.sparse import csc_matrix, csr_matrix, lil_matrix\n",
    "from sklearn.preprocessing import Binarizer\n",
    "np.seterr(all='warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:21:55.885705",
     "start_time": "2016-11-08T14:21:52.946733"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('List of arrays in this file: \\n', [u'HDF5.name___X', u'HDF5.name___y'])\n",
      "(60000, 784) (60000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2117.6342254841179"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf =  h5py.File('mnist.h5','r')\n",
    "print('List of arrays in this file: \\n', hf.keys())\n",
    "X = np.array(hf.get('HDF5.name___X'))\n",
    "y = np.array(hf.get('HDF5.name___y'))\n",
    "\n",
    "print X.shape, y.shape\n",
    "hf.close()\n",
    "np.linalg.norm(X, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Helper methods for simple case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:21:55.891637",
     "start_time": "2016-11-08T14:21:55.887305"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.fixes import expit    \n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "\n",
    "def sig_means(x, b, W):\n",
    "    a = safe_sparse_dot(x, W.T) + b\n",
    "    return expit(a, out=a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-20T17:56:14.156132",
     "start_time": "2016-10-20T17:56:14.153682"
    }
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:21:57.298554",
     "start_time": "2016-11-08T14:21:55.893724"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load emf_rbm.py\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.externals.six.moves import xrange\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.utils import gen_even_slices\n",
    "from sklearn.utils import issparse\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "from sklearn.utils.fixes import expit  # logistic function  \n",
    "from sklearn.utils.extmath import safe_sparse_dot, log_logistic, softmax\n",
    "\n",
    "class EMF_RBM(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extended Mean Field Restricted Boltzmann Machine (RBM).\n",
    "    A Restricted Boltzmann Machine with binary visible units and\n",
    "    binary hidden units. Parameters are estimated using the Extended Mean\n",
    "    Field model, based on the TAP equations\n",
    "    Read more in the :ref:`User Guide <rbm>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_components : int, optional\n",
    "        Number of binary hidden units.\n",
    "    learning_rate : float, optional\n",
    "        The learning rate for weight updates. It is *highly* recommended\n",
    "        to tune this hyper-parameter. Reasonable values are in the\n",
    "        10**[0., -3.] range.\n",
    "    batch_size : int, optional\n",
    "        Number of examples per minibatch.\n",
    "    momentum : float, optional\n",
    "        gradient momentum parameter\n",
    "    decay : float, optional\n",
    "        decay for weight update regularizer\n",
    "    weight_decay: string, optional []'L1', 'L2', None]\n",
    "        weight update regularizer\n",
    "\n",
    "    neq_steps: int, optional\n",
    "        Number of equilibration steps\n",
    "    n_iter : int, optional\n",
    "        Number of iterations/sweeps over the training dataset to perform\n",
    "        during training.\n",
    "    sigma: float, optional\n",
    "        variance of initial W weight matrix\n",
    "    thresh: float, optional\n",
    "        threshold for values in W weight matrix, vectors\n",
    "    verbose : int, optional\n",
    "        The verbosity level. The default, zero, means silent mode.\n",
    "    random_state : integer or numpy.RandomState, optional\n",
    "        A random number generator instance to define the state of the\n",
    "        random permutations generator. If an integer is given, it fixes the\n",
    "        seed. Defaults to the global numpy random number generator.\n",
    "    Attributes\n",
    "    ----------\n",
    "    h_bias : array-like, shape (n_components,)\n",
    "        Biases of the hidden units.\n",
    "    v_bias : array-like, shape (n_features,)\n",
    "        Biases of the visible units.\n",
    "    W : array-like, shape (n_components, n_features)\n",
    "        Weight matrix, where n_features in the number of\n",
    "        visible units and n_components is the number of hidden units.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "    >>> model = EMF_RBM(n_components=2)\n",
    "    >>> model.fit(X)\n",
    "    EmfRBM(batch_size=10, learning_rate=0.1, n_components=2, n_iter=10,\n",
    "           random_state=None, verbose=0)\n",
    "    References\n",
    "    ----------\n",
    "    [1] Marylou GabrieÂ´, Eric W. Tramel1 and Florent Krzakala1, \n",
    "        Training Restricted Boltzmann Machines via the Thouless-Anderson-Palmer Free Energy\n",
    "        https://arxiv.org/pdf/1506.02914\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=256, learning_rate=0.005, batch_size=100, sigma=0.001, neq_steps = 3,\n",
    "                 n_iter=20, verbose=0, random_state=None, momentum = 0.5, decay = 0.01, weight_decay='L1', thresh=1e-8):\n",
    "        self.n_components = n_components\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.n_iter = n_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.sigma = sigma\n",
    "        self.neq_steps = neq_steps\n",
    "\n",
    "        # learning rate / mini_batch\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # threshold for floats\n",
    "        self.thresh = thresh\n",
    "\n",
    "        # store in case we want to reset\n",
    "        self.random_state = random_state\n",
    "        \n",
    "\n",
    "        # self.random_state_ = random_state\n",
    "        # always start with new random state\n",
    "        self.random_state = check_random_state(random_state)\n",
    "        \n",
    "        # h bias\n",
    "        self.h_bias = np.zeros(self.n_components, )\n",
    "        self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "        # moved to fit\n",
    "        \n",
    "        self.W = None\n",
    "        self.dW_prev = None\n",
    "        self.W2 = None\n",
    "        self.v_bias = None\n",
    "        \n",
    "\n",
    "    def init_weights(self, X):\n",
    "        \"\"\" If the user specifies the training dataset, it can be useful to                                                                                   \n",
    "        initialize the visibile biases according to the empirical expected                                                                                \n",
    "        feature values of the training data.                                                                                                              \n",
    "\n",
    "        TODO: Generalize this biasing. Currently, the biasing is only written for                                                                         \n",
    "               the case of binary RBMs.\n",
    "        \"\"\"\n",
    "        # \n",
    "        eps = self.thresh\n",
    "\n",
    "        # Mean across  samples \n",
    "        if issparse(X):\n",
    "            probVis = csr_matrix.mean(X, axis=0)\n",
    "        else:\n",
    "            probVis = np.mean(X,axis=0)            \n",
    "\n",
    "        # safe for CSR / sparse mats ?\n",
    "        # do we need it if we use softmax ?\n",
    "        probVis[probVis < eps] = eps            # Some regularization (avoid Inf/NaN)  \n",
    "        #probVis[probVis < (1.0-eps)] = (1.0-eps)   \n",
    "        self.v_bias = np.log(probVis / (1.0-probVis)) # Biasing as the log-proportion\n",
    "        \n",
    "        # (does not work)\n",
    "        # self.v_bias = softmax(probVis)\n",
    "        \n",
    "        # initialize arrays to 0\n",
    "        self.W = np.asarray(\n",
    "            self.random_state.normal(\n",
    "                0,\n",
    "                self.sigma,\n",
    "                (self.n_components, X.shape[1])\n",
    "            ),\n",
    "            order='fortran')\n",
    "\n",
    "        self.dW_prev = np.zeros_like(self.W)\n",
    "        self.W2 = self.W*self.W\n",
    "        return 0\n",
    "\n",
    "\n",
    "    def sample_layer(self, layer):\n",
    "        \"\"\"Sample from the conditional distribution P(h|v) or P(v|h)\"\"\"\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        sample = (self.random_state.random_sample(size=layer.shape) < layer) \n",
    "        return sample\n",
    "\n",
    "    def _sample_hiddens(self, v):\n",
    "        \"\"\"Sample from the conditional distribution P(h|v).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer to sample from.\n",
    "        Returns\n",
    "        -------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer.\n",
    "        \"\"\"\n",
    "        return self.sample_layer(self._mean_hiddens(v))\n",
    "\n",
    "    def _mean_hiddens(self, v):\n",
    "        \"\"\"Computes the conditional probabilities P(h=1|v).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Corresponding mean field values for the hidden layer.\n",
    "        \"\"\"\n",
    "        p = safe_sparse_dot(v, self.W.T) + self.h_bias\n",
    "        return expit(p, out=p)\n",
    "\n",
    "    def _sample_visibles(self, h):\n",
    "        \"\"\"Sample from the distribution P(v|h).\n",
    "        Parameters\n",
    "        ----------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Values of the hidden layer to sample from.\n",
    "        Returns\n",
    "        -------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        \"\"\"\n",
    "        return sample_layer(self._mean_visible(h))\n",
    "\n",
    "    def _mean_visibles(self, h):\n",
    "        \"\"\"Computes the conditional probabilities P(v=1|h).\n",
    "        Parameters\n",
    "        ----------\n",
    "        h : array-like, shape (n_samples, n_components)\n",
    "            Corresponding mean field values for the hidden layer.\n",
    "        Returns\n",
    "        -------\n",
    "         v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.     \n",
    "        \"\"\"\n",
    "        #p = np.dot(h, self.W) + self.v_bias\n",
    "        p = safe_sparse_dot(h, W) + self.v_bias\n",
    "        return expit(p, out=p)\n",
    "\n",
    "    def sigma_means(self, x, b, W):\n",
    "        \"\"\"helper class for computing Wx+b \"\"\"\n",
    "        a = safe_sparse_dot(x, W.T) + b\n",
    "        return expit(a, out=a)\n",
    "\n",
    "    def init_batch(self, vis):\n",
    "        \"\"\"initialize the batch for EMF only\"\"\"\n",
    "        v_pos = vis\n",
    "        v_init = v_pos\n",
    "\n",
    "        h_pos = self._mean_hiddens(v_pos)\n",
    "        h_init = h_pos\n",
    "\n",
    "        return v_pos, h_pos, v_init, h_init\n",
    "\n",
    "    def equilibrate(self, v0, h0, iters=3):\n",
    "        \"\"\"Run iters steps of the TAP fixed point equations\"\"\"\n",
    "        mv = v0\n",
    "        mh = h0\n",
    "     \n",
    "        for i in range(iters):\n",
    "            mv = 0.5 *self.mv_update(mv, mh) + 0.5*mv\n",
    "            mh = 0.5 *self.mh_update(mv, mh) + 0.5*mh\n",
    "        return mv, mh\n",
    "\n",
    "    def mv_update(self, v, h):  \n",
    "        \"\"\"update TAP visbile magnetizations, to second order\"\"\"\n",
    "        \n",
    "        # a = np.dot(h, self.W) + self.v_bias\n",
    "        a = safe_sparse_dot(h, self.W) + self.v_bias\n",
    "\n",
    "        h_fluc = h-np.multiply(h,h)\n",
    "        #a += h_fluc.dot(self.W2)*(0.5-v)\n",
    "        \n",
    "        # 0.5-v is elementwise => dense\n",
    "        if issparse(v):\n",
    "            v_half = (0.5-v.todense())\n",
    "        else:\n",
    "             v_half = (0.5-v)\n",
    "            \n",
    "        a += np.multiply(safe_sparse_dot(h_fluc,self.W2), v_half)\n",
    "        return expit(a, out=a)\n",
    "\n",
    "    def mh_update(self, v, h):\n",
    "        \"\"\"update TAP hidden magnetizations, to second order\"\"\"\n",
    "        a = safe_sparse_dot(v, self.W.T) + self.h_bias\n",
    " \n",
    "        v_fluc = (v-(np.multiply(v,v)))\n",
    "        #a += (v-v*v).dot((self.W2).T)*(0.5-h)\n",
    "        \n",
    "        if issparse(h):\n",
    "            h_half = (0.5-h.to_dense())\n",
    "        else:        \n",
    "            h_half = (0.5-h)\n",
    "            \n",
    "        a += np.multiply(safe_sparse_dot(v_fluc,self.W2.T),h_half)\n",
    "\n",
    "        return expit(a, out=a)\n",
    "\n",
    "\n",
    "    def weight_gradient(self, v_pos, h_pos ,v_neg, h_neg):\n",
    "        \"\"\"compute weight gradient of the TAP Free Energy, to second order\"\"\"\n",
    "        # naive  / mean field\n",
    "        dW = safe_sparse_dot(v_pos.T, h_pos, dense_output=True).T - np.dot(h_neg.T, v_neg)\n",
    "        \n",
    "        # tap2 correction\n",
    "        #  elementwise multiplies\n",
    "        h_fluc = (h_neg - np.multiply(h_neg,h_neg)).T\n",
    "        v_fluc = (v_neg - np.multiply(v_neg,v_neg))\n",
    "        #  dW_tap2 = h_fluc.dot(v_fluc)*self.W\n",
    "        dW_tap2 = np.multiply(safe_sparse_dot(h_fluc,v_fluc),self.W)\n",
    "\n",
    "        dW -= dW_tap2\n",
    "        return dW\n",
    "\n",
    "    def score_samples(self, X):\n",
    "        \"\"\"Compute the pseudo-likelihood of X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "        Returns\n",
    "        -------\n",
    "        pseudo_likelihood : array-like, shape (n_samples,)\n",
    "            Value of the pseudo-likelihood (proxy for likelihood).\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes the TAP Free Energy on X,\n",
    "        then on a randomly corrupted version of X, and\n",
    "        returns the log of the logistic function of the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')\n",
    "        v, v_ = self._corrupt_data(v)       \n",
    "\n",
    "        fe = self._free_energy(v)\n",
    "        fe_ = self._free_energy(v_)\n",
    "        return v.shape[1] * log_logistic(fe_ - fe)\n",
    "    \n",
    "    \n",
    "    def score_samples_TAP(self, X):\n",
    "        \"\"\"Compute the pseudo-likelihood of X using second order TAP\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "        Returns\n",
    "        -------\n",
    "        pseudo_likelihood : array-like, shape (n_samples,)\n",
    "            Value of the pseudo-likelihood (proxy for likelihood).\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes the TAP Free Energy on X,\n",
    "        then on a randomly corrupted version of X, and\n",
    "        returns the log of the logistic function of the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')      \n",
    "        v, v_ = self._corrupt_data(v)       \n",
    "\n",
    "        fe = self._free_energy_TAP(v)\n",
    "        fe_ = self._free_energy_TAP(v_)\n",
    "        return v.shape[1] * log_logistic(fe_ - fe)\n",
    "    \n",
    "    def _corrupt_data(self, v):\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        \"\"\"Randomly corrupt one feature in each sample in v.\"\"\"\n",
    "        ind = (np.arange(v.shape[0]),\n",
    "               self.random_state.randint(0, v.shape[1], v.shape[0]))\n",
    "        if issparse(v):\n",
    "            data = -2 * v[ind] + 1\n",
    "            v_ = v + sp.csr_matrix((data.A.ravel(), ind), shape=v.shape)\n",
    "        else:\n",
    "            v_ = v.copy()\n",
    "            v_[ind] = 1 - v_[ind]\n",
    "        return v, v_\n",
    "    \n",
    "    \n",
    "    def score_samples_entropy(self, X):\n",
    "        \"\"\"Compute the entropy of X\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Values of the visible layer. Must be all-boolean (not checked).\n",
    "        Returns\n",
    "        -------\n",
    "        entropy : array-like, shape (n_samples,)\n",
    "            Value of the entropy.\n",
    "        Notes\n",
    "        -----\n",
    "        This method is not deterministic: it computes the entropy on X,\n",
    "        then on a randomly corrupted version of X, and returns the difference.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        v = check_array(X, accept_sparse='csr')\n",
    "        v, v_ = self._corrupt_data(v)       \n",
    "\n",
    "        s = self._entropy(v)\n",
    "        s_ = self._entropy(v_)\n",
    "        return v.shape[1] * (s_ - s)\n",
    "\n",
    "    \n",
    "        #TODO: run per column\n",
    "    def _denoise(self, m, eps=1.0e-8):\n",
    "        \"\"\"denoise magnetization\"\"\"\n",
    "        m = np.maximum(m,eps)\n",
    "        m = np.minimum(m,1.0-eps)\n",
    "        return m\n",
    "\n",
    "\n",
    "    def _U_naive_TAP(self, v):\n",
    "        \"\"\"Computes the  Mean Field TAP Energy E(v) \n",
    "        Parameters. This is also U_Naive in the TAP FE\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        U_naive : array-like, shape (n_samples,)\n",
    "            The value of the mean field component of the TAP free energy.\n",
    "        \"\"\"\n",
    "        h = self._mean_hiddens(v)\n",
    "        mv, mh = self.equilibrate(v, h, iters=self.neq_steps)\n",
    "           \n",
    "        mv = self._denoise(mv)\n",
    "        mh = self._denoise(mh)\n",
    "    \n",
    "        # sum over nodes: axis=1\n",
    "        \n",
    "        U_naive = (-safe_sparse_dot(mv, self.v_bias) \n",
    "                    -safe_sparse_dot(mh, self.h_bias) \n",
    "                        -(mv.dot(self.W.T)*(mh)).sum(axis=1))         \n",
    "\n",
    "        return U_naive\n",
    "            \n",
    "    def _free_energy_TAP(self, v):\n",
    "        \"\"\"Computes the TAP Free Energy F(v) to second order\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        #fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "        #        - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "        #                       + self.h_bias).sum(axis=1))\n",
    "        \n",
    "        h = self._mean_hiddens(v)\n",
    "        mv, mh = self.equilibrate(v, h, iters=self.neq_steps)\n",
    "           \n",
    "        mv = self._denoise(mv)\n",
    "        mh = self._denoise(mh)\n",
    "    \n",
    "        # sum over nodes: axis=1\n",
    "        \n",
    "        U_naive = (-safe_sparse_dot(mv, self.v_bias) \n",
    "                    -safe_sparse_dot(mh, self.h_bias) \n",
    "                        -(mv.dot(self.W.T)*(mh)).sum(axis=1))     \n",
    "\n",
    "        Entropy = ( -(mv*np.log(mv)+(1.0-mv)*np.log(1.0-mv)).sum(axis=1)  \n",
    "                    -(mh*np.log(mh)+(1.0-mh)*np.log(1.0-mh)).sum(axis=1) )\n",
    "                   \n",
    "        h_fluc = (mh - (mh*mh))\n",
    "        v_fluc = (mv - (mv*mv))\n",
    "\n",
    "        # if we do it this way, we need to normalize by 1/batch_size \n",
    "        # which we need to obtain from the W2 matrix\n",
    "        # (I think because of the double sum)\n",
    "        # this is not obvious in the paper...have to be very careful here\n",
    "        tap_norm = 1.0/float(mv.shape[0])\n",
    "        dW_tap2 = h_fluc.dot(self.W2).dot(v_fluc.T)\n",
    "\n",
    "        # julia way, does not require extra norm, but maybe slower ?\n",
    "        # dW_tap2 = h_fluc.dot(self.W2)*v_fluc\n",
    "        Onsager = (-0.5*dW_tap2).sum(axis=1)*tap_norm\n",
    "        fe_tap = U_naive + Onsager - Entropy\n",
    "        \n",
    "        print \"S \", np.mean(Entropy)\n",
    "        print \"U \", np.mean(U_naive)\n",
    "        print \"O \", np.mean(Onsager)\n",
    "\n",
    "        return fe_tap \n",
    "\n",
    "\n",
    "    \n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"Computes the RBM Free Energy F(v) Parameters.  \n",
    "        (No mean field h values necessary)\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1) )\n",
    "\n",
    "        return fe \n",
    "            \n",
    "\n",
    "    def _entropy(self, v):\n",
    "        \"\"\"Computes the TAP Entropy (S) , from an equilibration step\n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        entropy : array-like, shape (n_samples,)\n",
    "            The value of the entropy.\n",
    "        \"\"\"\n",
    "         \n",
    "        h = self._mean_hiddens(v)\n",
    "        mv, mh = self.equilibrate(v, h, iters=self.neq_steps)\n",
    "\n",
    "        mv = self._denoise(mv)\n",
    "        mh = self._denoise(mh)\n",
    "\n",
    "        # appears to be wrong ?  unsure why ?  maybe because it is not denoised !!!\n",
    "        Entropy = ( -(mv*np.log(mv)+(1.0-mv)*np.log(1.0-mv)).sum(axis=1)  \n",
    "                    -(mh*np.log(mh)+(1.0-mh)*np.log(1.0-mh)).sum(axis=1)  )\n",
    "                         \n",
    "        return Entropy\n",
    "\n",
    "\n",
    "    \n",
    "    def _free_energy(self, v):\n",
    "        \"\"\"Computes the RBM Free Energy F(v) \n",
    "        Parameters\n",
    "        ----------\n",
    "        v : array-like, shape (n_samples, n_features)\n",
    "            Values of the visible layer.\n",
    "        Returns\n",
    "        -------\n",
    "        free_energy : array-like, shape (n_samples,)\n",
    "            The value of the free energy.\n",
    "        \"\"\"\n",
    "        fe = (- safe_sparse_dot(v, self.v_bias)\n",
    "                - np.logaddexp(0, safe_sparse_dot(v, self.W.T)\n",
    "                               + self.h_bias).sum(axis=1) )\n",
    "\n",
    "        return fe \n",
    "\n",
    "    \n",
    "    def partial_fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X which should contain a partial\n",
    "        segment of the data.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        Returns\n",
    "        -------\n",
    "        self : EMF_RBM\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        if not hasattr(self, 'random_state_'):\n",
    "            self.random_state_ = check_random_state(self.random_state)\n",
    "        if not hasattr(self, 'W'):\n",
    "            self.W = np.asarray(\n",
    "                self.random_state_.normal(\n",
    "                    0,\n",
    "                    0.01,\n",
    "                    (self.n_components, X.shape[1])\n",
    "                ),\n",
    "                order='F')\n",
    "        if not hasattr(self, 'h_bias'):\n",
    "            self.h_bias = np.zeros(self.n_components, )\n",
    "        if not hasattr(self, 'v_bias'):\n",
    "            self.v_bias = np.zeros(X.shape[1], )\n",
    "\n",
    "        # not used ?\n",
    "        #if not hasattr(self, 'h_samples_'):\n",
    "        #    self.h_samples_ = np.zeros((self.batch_size, self.n_components))\n",
    "\n",
    "        self._fit(X)\n",
    "\n",
    "    def _fit(self, v_pos):\n",
    "        \"\"\"Inner fit for one mini-batch.\n",
    "        Adjust the parameters to maximize the likelihood of v using\n",
    "        Extended Mean Field theory (second order TAP equations).\n",
    "        Parameters\n",
    "        ----------\n",
    "        v_pos : array-like, shape (n_samples, n_features)\n",
    "            The data to use for training.\n",
    "        \"\"\"\n",
    "        X_batch = v_pos\n",
    "        lr = float(self.learning_rate) / X_batch.shape[0]\n",
    "        decay = self.decay\n",
    "\n",
    "        v_pos, h_pos, v_init, h_init = self.init_batch(X_batch)\n",
    "              \n",
    "        a = safe_sparse_dot(h_init, self.W, dense_output=True) + self.v_bias\n",
    "        a = expit(a, out=a)\n",
    "\n",
    "        # get_negative_samples\n",
    "        v_neg, h_neg = self.equilibrate(v_init, h_init, iters=self.neq_steps) \n",
    "        \n",
    "        # basic gradient\n",
    "        dW = self.weight_gradient(v_pos, h_pos ,v_neg, h_neg) \n",
    "\n",
    "        # regularization based on weight decay\n",
    "        #  similar to momentum >\n",
    "        if self.weight_decay == \"L1\":\n",
    "            dW -= decay * np.sign(self.W)\n",
    "        elif self.weight_decay == \"L2\":\n",
    "            dW -= decay * self.W\n",
    "\n",
    "        # can we use BLAS here ?\n",
    "        # momentum\n",
    "        # note:  what do we do if lr changes per step ? not ready yet\n",
    "        dW += self.momentum * self.dW_prev  \n",
    "        # update\n",
    "        self.W += lr * dW \n",
    "\n",
    "        # storage for next iteration\n",
    "\n",
    "        # is this is a memory killer \n",
    "        self.dW_prev =  dW  \n",
    "        \n",
    "        # is this wasteful...can we avoid storing 2X the W mat ?\n",
    "        # elementwise multiply\n",
    "        self.W2 = np.multiply(self.W,self.W)\n",
    "\n",
    "        # update bias terms\n",
    "        #   csr matrix sum is screwy, returns [[1,self.n_components]] 2-d array  \n",
    "        #   so I always use np.asarray(X.sum(axis=0)).squeeze()\n",
    "        #   although (I think) this could be optimized\n",
    "        self.v_bias += lr * (np.asarray(v_pos.sum(axis=0)).squeeze() - np.asarray(v_neg.sum(axis=0)).squeeze())\n",
    "        self.h_bias += lr * (np.asarray(h_pos.sum(axis=0)).squeeze() - np.asarray(h_neg.sum(axis=0)).squeeze())\n",
    "\n",
    "        return 0\n",
    "\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the model to the data X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        Returns\n",
    "        -------\n",
    "        self : EMF_RBM\n",
    "            The fitted model.\n",
    "        \"\"\"\n",
    "        verbose = self.verbose\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        self.random_state = check_random_state(self.random_state)\n",
    "        \n",
    "        self.init_weights(X)\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        n_batches = int(np.ceil(float(n_samples) / self.batch_size))\n",
    "        \n",
    "\n",
    "        batch_slices = list(gen_even_slices(n_batches * self.batch_size,\n",
    "                                            n_batches, n_samples))\n",
    "        \n",
    "        begin = time.time()\n",
    "        for iteration in xrange(1, self.n_iter + 1):\n",
    "            for batch_slice in batch_slices:\n",
    "                self._fit(X[batch_slice])\n",
    "\n",
    "            if verbose:\n",
    "                end = time.time()\n",
    "                print(\"[%s] Iteration %d, pseudo-likelihood = %.2f,\"\n",
    "                      \" time = %.2fs\"\n",
    "                      % (type(self).__name__, iteration,\n",
    "                         self.score_samples(X).mean(), end - begin))\n",
    "                begin = end\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Compute the hidden layer activation probabilities, P(h=1|v=X).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix} shape (n_samples, n_features)\n",
    "            The data to be transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        h : array, shape (n_samples, n_components)\n",
    "            Latent representations of the data.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, \"W\")\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr', dtype=np.float64)\n",
    "        return self._mean_hiddens(X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:26:51.747963",
     "start_time": "2016-11-08T14:21:57.300285"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EMF_RBM] Iteration 1, pseudo-likelihood = -186.66, time = 10.84s\n",
      "[EMF_RBM] Iteration 2, pseudo-likelihood = -150.33, time = 13.96s\n",
      "[EMF_RBM] Iteration 3, pseudo-likelihood = -134.36, time = 14.33s\n",
      "[EMF_RBM] Iteration 4, pseudo-likelihood = -122.58, time = 14.44s\n",
      "[EMF_RBM] Iteration 5, pseudo-likelihood = -114.54, time = 15.74s\n",
      "[EMF_RBM] Iteration 6, pseudo-likelihood = -108.11, time = 16.73s\n",
      "[EMF_RBM] Iteration 7, pseudo-likelihood = -102.52, time = 15.77s\n",
      "[EMF_RBM] Iteration 8, pseudo-likelihood = -97.48, time = 14.92s\n",
      "[EMF_RBM] Iteration 9, pseudo-likelihood = -96.36, time = 13.91s\n",
      "[EMF_RBM] Iteration 10, pseudo-likelihood = -94.42, time = 14.69s\n",
      "[EMF_RBM] Iteration 11, pseudo-likelihood = -91.45, time = 14.96s\n",
      "[EMF_RBM] Iteration 12, pseudo-likelihood = -88.43, time = 14.94s\n",
      "[EMF_RBM] Iteration 13, pseudo-likelihood = -85.54, time = 14.79s\n",
      "[EMF_RBM] Iteration 14, pseudo-likelihood = -85.22, time = 14.07s\n",
      "[EMF_RBM] Iteration 15, pseudo-likelihood = -81.19, time = 14.21s\n",
      "[EMF_RBM] Iteration 16, pseudo-likelihood = -84.18, time = 16.15s\n",
      "[EMF_RBM] Iteration 17, pseudo-likelihood = -81.33, time = 15.47s\n",
      "[EMF_RBM] Iteration 18, pseudo-likelihood = -79.63, time = 14.04s\n",
      "[EMF_RBM] Iteration 19, pseudo-likelihood = -79.02, time = 14.10s\n",
      "[EMF_RBM] Iteration 20, pseudo-likelihood = -78.01, time = 14.20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EMF_RBM(batch_size=100, decay=0.01, learning_rate=0.005, momentum=0.5,\n",
       "    n_components=256, n_iter=20, neq_steps=3,\n",
       "    random_state=<mtrand.RandomState object at 0x10b9535a0>, sigma=0.001,\n",
       "    thresh=1e-08, verbose=True, weight_decay='L1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emf_rbm = EMF_RBM(verbose=True)\n",
    "emf_rbm.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T13:42:53.473982",
     "start_time": "2016-11-08T13:42:53.469340"
    }
   },
   "source": [
    "### Mean field TAP Energies\n",
    "\n",
    "Not exactly Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:27:41.236530",
     "start_time": "2016-11-08T14:27:41.233474"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ids = np.random.choice(X.shape[0], size=6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:27:59.753176",
     "start_time": "2016-11-08T14:27:58.703651"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U = emf_rbm._U_naive_TAP(X[ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:32:29.917164",
     "start_time": "2016-11-08T14:32:29.912768"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:28:01.186684",
     "start_time": "2016-11-08T14:28:00.751816"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nST1nIpCknjMRSFLPmQgkqedMBJLUc/8ffAGULbOwBe8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1068314d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(U,100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T13:54:45.187108",
     "start_time": "2016-11-08T13:54:45.184452"
    }
   },
   "source": [
    "### Plain Mean field Energies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:28:09.747812",
     "start_time": "2016-11-08T14:28:09.679136"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = sig_means(X[ids],emf_rbm.h_bias,emf_rbm.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:31:56.272616",
     "start_time": "2016-11-08T14:31:56.214258"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Emf = safe_sparse_dot(p, emf_rbm.h_bias) + safe_sparse_dot(X[ids], emf_rbm.v_bias)  + (X[ids].dot(emf_rbm.W.T)*p).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:32:01.904695",
     "start_time": "2016-11-08T14:32:01.901151"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Emf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-11-08T14:32:48.307072",
     "start_time": "2016-11-08T14:32:46.359981"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEACAYAAAC+gnFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128262490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(-Emf,100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are clearly NOT Guassian distributions, and I have a feeling that these are ground state energies, and subject to EVS, not the REM.  And that the REM has to be determined by looking at all Energies during all training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
